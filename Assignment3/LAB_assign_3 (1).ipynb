{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjKSGwAF27Z3",
        "outputId": "863b404f-4913-4cef-85c2-0e6d76e36642"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct  2 07:38:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8QmacOO4XcC",
        "outputId": "dd6d7e99-1060-41e7-eee4-c9387e82af3b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > cuda_vector_demo.cu <<'EOF'\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define N 1024\n",
        "\n",
        "#define CHECK(call) { \\\n",
        "    cudaError_t e = (call); \\\n",
        "    if (e != cudaSuccess) { \\\n",
        "        fprintf(stderr, \"CUDA Error: %s (file %s, line %d)\\n\", cudaGetErrorString(e), __FILE__, __LINE__); \\\n",
        "        exit(1); \\\n",
        "    } \\\n",
        "}\n",
        "\n",
        "// ========================== KERNELS =============================\n",
        "\n",
        "__global__ void kernel_serial(int *A, int *B, int *C, int *D) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N) {\n",
        "        A[i] = i;\n",
        "        B[i] = 2*i;\n",
        "        C[i] = i*i;\n",
        "        D[i] = A[i] + B[i] + C[i]; // expect 9*i^2\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void kernel_partition(int *A, int *B, int *C, int *D, int offset) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int gi = offset + i;\n",
        "    if (gi < N) {\n",
        "        A[gi] = gi;\n",
        "        B[gi] = 2*gi;\n",
        "        C[gi] = gi*gi;\n",
        "        D[gi] = A[gi] + B[gi] + C[gi];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void kernel_badRace(int *A, int *B, int *C, int *D) {\n",
        "    int i = threadIdx.x;\n",
        "    A[0] = i;\n",
        "    B[0] = 2*i;\n",
        "    C[0] = i*i;\n",
        "    D[0] = A[0] + B[0] + C[0];\n",
        "}\n",
        "\n",
        "__global__ void kernel_goodRace(int *A, int *B, int *C, int *D) {\n",
        "    int i = threadIdx.x;\n",
        "    __shared__ int tmpA, tmpB, tmpC;\n",
        "    if (i == 0) {\n",
        "        tmpA = 0; tmpB = 0; tmpC = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "    atomicAdd(&tmpA, i);\n",
        "    atomicAdd(&tmpB, 2*i);\n",
        "    atomicAdd(&tmpC, i*i);\n",
        "    __syncthreads();\n",
        "    if (i == 0) {\n",
        "        D[0] = tmpA + tmpB + tmpC;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void kernel_map(int *out) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    out[i] = i;\n",
        "}\n",
        "\n",
        "// ============== Reduction (Fixed for CUDA 12.5) ==================\n",
        "\n",
        "__global__ void reductionKernel(int *D, unsigned long long *sum) {\n",
        "    __shared__ int temp[1024];\n",
        "    int tid = threadIdx.x;\n",
        "    int i = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    temp[tid] = D[i];\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            temp[tid] += temp[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        atomicAdd(sum, (unsigned long long)temp[0]); // FIXED\n",
        "    }\n",
        "}\n",
        "\n",
        "// ========================== MAIN =============================\n",
        "\n",
        "int main() {\n",
        "    printf(\"CUDA Vector Ops Demo (N=%d)\\n\\n\", N);\n",
        "\n",
        "    int *dA, *dB, *dC, *dD;\n",
        "    CHECK(cudaMalloc(&dA, N*sizeof(int)));\n",
        "    CHECK(cudaMalloc(&dB, N*sizeof(int)));\n",
        "    CHECK(cudaMalloc(&dC, N*sizeof(int)));\n",
        "    CHECK(cudaMalloc(&dD, N*sizeof(int)));\n",
        "\n",
        "    // ===== Demo 1: Serial execution =====\n",
        "    printf(\"=== Demo 1: Serial execution on default stream ===\\n\");\n",
        "    kernel_serial<<<N/256, 256>>>(dA, dB, dC, dD);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    int hD[8];\n",
        "    CHECK(cudaMemcpy(hD, dD, 8*sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    for (int i=0; i<8; i++) {\n",
        "        printf(\"D[%d] = %d\\n\", i, hD[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // ===== Demo 2: Streams + partitioning =====\n",
        "    printf(\"=== Demo 2: Streams + partitioning ===\\n\");\n",
        "    cudaStream_t s1, s2;\n",
        "    cudaStreamCreate(&s1);\n",
        "    cudaStreamCreate(&s2);\n",
        "\n",
        "    kernel_partition<<<N/512, 256, 0, s1>>>(dA, dB, dC, dD, 0);\n",
        "    kernel_partition<<<N/512, 256, 0, s2>>>(dA, dB, dC, dD, N/2);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    int hD2[8];\n",
        "    CHECK(cudaMemcpy(hD2, dD, 4*sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    CHECK(cudaMemcpy(hD2+4, dD+512, 4*sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    for (int i=0; i<8; i++) {\n",
        "        printf(\"D[%d] = %d\\n\", (i<4)?i:512+(i-4), hD2[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // ===== Demo 3: Race condition =====\n",
        "    printf(\"=== Demo 3: Race condition + fix ===\\n\");\n",
        "    kernel_badRace<<<1, 256>>>(dA, dB, dC, dD);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaMemcpy(hD, dD, 5*sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    printf(\"Bad (racy) results sample:\\n\");\n",
        "    for (int i=0; i<5; i++) printf(\"D[%d] = %d\\n\", i, hD[i]);\n",
        "\n",
        "    kernel_goodRace<<<1, 256>>>(dA, dB, dC, dD);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "    CHECK(cudaMemcpy(hD, dD, 5*sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    printf(\"Fixed (event-ordered) results sample:\\n\");\n",
        "    for (int i=0; i<5; i++) printf(\"D[%d] = %d\\n\", i, hD[i]);\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // ===== Demo 4: cudaMemcpyAsync =====\n",
        "    printf(\"=== Demo 4: cudaMemcpyAsync + stream sync ===\\n\");\n",
        "    int *hD_pinned;\n",
        "    CHECK(cudaMallocHost(&hD_pinned, 5*sizeof(int)));\n",
        "    for (int i=0; i<5; i++) hD_pinned[i] = -999;\n",
        "\n",
        "    cudaMemcpyAsync(hD_pinned, dD, 5*sizeof(int), cudaMemcpyDeviceToHost, s1);\n",
        "    printf(\"Without synchronization: may print stale values:\\n\");\n",
        "    for (int i=0; i<5; i++) printf(\"hD_pinned[%d] = %d\\n\", i, hD_pinned[i]);\n",
        "\n",
        "    cudaStreamSynchronize(s1);\n",
        "    printf(\"After cudaStreamSynchronize (correct):\\n\");\n",
        "    for (int i=0; i<5; i++) printf(\"hD_pinned[%d] = %d\\n\", i, hD_pinned[i]);\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // ===== Demo 5: Thread hierarchy mapping =====\n",
        "    printf(\"=== Demo 5: Thread hierarchy mapping ===\\n\");\n",
        "    int *dOut, *hOut;\n",
        "    CHECK(cudaMalloc(&dOut, N*sizeof(int)));\n",
        "    hOut = (int*)malloc(N*sizeof(int));\n",
        "\n",
        "    kernel_map<<<1, N>>>(dOut);\n",
        "    CHECK(cudaMemcpy(hOut, dOut, 8*sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    printf(\"Mapping for <<<1, N>>> sample:\\n\");\n",
        "    for (int i=0; i<8; i++) printf(\"out[%d] = %d\\n\", i, hOut[i]);\n",
        "\n",
        "    kernel_map<<<N/32, 32>>>(dOut);\n",
        "    CHECK(cudaMemcpy(hOut, dOut, 8*sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    printf(\"Mapping for <<<N/32, 32>>> sample:\\n\");\n",
        "    for (int i=0; i<8; i++) {\n",
        "        printf(\"global index %d -> blockIdx=%d threadIdx=%d out[%d]=%d\\n\",\n",
        "               i, i/32, i%32, i, hOut[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // ===== Bonus: reduction =====\n",
        "    printf(\"=== Bonus: reduction (shared + atomic / partials) ===\\n\");\n",
        "    unsigned long long *dSum;\n",
        "    CHECK(cudaMalloc(&dSum, sizeof(unsigned long long)));\n",
        "    CHECK(cudaMemset(dSum, 0, sizeof(unsigned long long)));\n",
        "\n",
        "    reductionKernel<<<N/1024, 1024>>>(dD, dSum);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    unsigned long long hSum;\n",
        "    CHECK(cudaMemcpy(&hSum, dSum, sizeof(unsigned long long), cudaMemcpyDeviceToHost));\n",
        "    printf(\"Sum via shared-block + atomicAdd: %llu\\n\", hSum);\n",
        "\n",
        "    long long cpuSum = 0;\n",
        "    int *hAll = (int*)malloc(N*sizeof(int));\n",
        "    CHECK(cudaMemcpy(hAll, dD, N*sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    for (int i=0; i<N; i++) cpuSum += hAll[i];\n",
        "    printf(\"CPU direct sum of D[]: %lld\\n\", cpuSum);\n",
        "\n",
        "    printf(\"\\nAll demos completed.\\n\");\n",
        "    return 0;\n",
        "}\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "YY0iuiXr5IIz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "nvcc -arch=sm_75 -O2 cuda_vector_demo.cu -o demo\n",
        "./demo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrGTvVue5J_B",
        "outputId": "9fe4e566-0941-4e7c-c72b-b2c5d4ff1324"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Vector Ops Demo (N=1024)\n",
            "\n",
            "=== Demo 1: Serial execution on default stream ===\n",
            "D[0] = 0\n",
            "D[1] = 4\n",
            "D[2] = 10\n",
            "D[3] = 18\n",
            "D[4] = 28\n",
            "D[5] = 40\n",
            "D[6] = 54\n",
            "D[7] = 70\n",
            "\n",
            "=== Demo 2: Streams + partitioning ===\n",
            "D[0] = 0\n",
            "D[1] = 4\n",
            "D[2] = 10\n",
            "D[3] = 18\n",
            "D[512] = 263680\n",
            "D[513] = 264708\n",
            "D[514] = 265738\n",
            "D[515] = 266770\n",
            "\n",
            "=== Demo 3: Race condition + fix ===\n",
            "Bad (racy) results sample:\n",
            "D[0] = 9504\n",
            "D[1] = 4\n",
            "D[2] = 10\n",
            "D[3] = 18\n",
            "D[4] = 28\n",
            "Fixed (event-ordered) results sample:\n",
            "D[0] = 5657600\n",
            "D[1] = 4\n",
            "D[2] = 10\n",
            "D[3] = 18\n",
            "D[4] = 28\n",
            "\n",
            "=== Demo 4: cudaMemcpyAsync + stream sync ===\n",
            "Without synchronization: may print stale values:\n",
            "hD_pinned[0] = -999\n",
            "hD_pinned[1] = -999\n",
            "hD_pinned[2] = -999\n",
            "hD_pinned[3] = -999\n",
            "hD_pinned[4] = -999\n",
            "After cudaStreamSynchronize (correct):\n",
            "hD_pinned[0] = 5657600\n",
            "hD_pinned[1] = 4\n",
            "hD_pinned[2] = 10\n",
            "hD_pinned[3] = 18\n",
            "hD_pinned[4] = 28\n",
            "\n",
            "=== Demo 5: Thread hierarchy mapping ===\n",
            "Mapping for <<<1, N>>> sample:\n",
            "out[0] = 0\n",
            "out[1] = 1\n",
            "out[2] = 2\n",
            "out[3] = 3\n",
            "out[4] = 4\n",
            "out[5] = 5\n",
            "out[6] = 6\n",
            "out[7] = 7\n",
            "Mapping for <<<N/32, 32>>> sample:\n",
            "global index 0 -> blockIdx=0 threadIdx=0 out[0]=0\n",
            "global index 1 -> blockIdx=0 threadIdx=1 out[1]=1\n",
            "global index 2 -> blockIdx=0 threadIdx=2 out[2]=2\n",
            "global index 3 -> blockIdx=0 threadIdx=3 out[3]=3\n",
            "global index 4 -> blockIdx=0 threadIdx=4 out[4]=4\n",
            "global index 5 -> blockIdx=0 threadIdx=5 out[5]=5\n",
            "global index 6 -> blockIdx=0 threadIdx=6 out[6]=6\n",
            "global index 7 -> blockIdx=0 threadIdx=7 out[7]=7\n",
            "\n",
            "=== Bonus: reduction (shared + atomic / partials) ===\n",
            "Sum via shared-block + atomicAdd: 364618752\n",
            "CPU direct sum of D[]: 364618752\n",
            "\n",
            "All demos completed.\n"
          ]
        }
      ]
    }
  ]
}